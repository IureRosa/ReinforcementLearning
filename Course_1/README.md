# Fundamentals of Reinforcement Learning

Reinforcement Learning is a subfield of Machine Learning, but is also a general purpose formalism for automated decision-making and AI. This course introduces you to statistical learning techniques where an agent explicitly takes actions and interacts with the world. Understanding the importance and challenges of learning agents that make decisions is of vital importance today, with more and more companies interested in interactive agents and intelligent decision-making. 

-This course introduces you to the fundamentals of Reinforcement Learning. When you finish this course, you will:

  - Formalize problems as Markov Decision Processes;
  - Understand basic exploration methods and the exploration/exploitation tradeoff;
  - Understand value functions, as a general-purpose tool for optimal decision-making;
  - Know how to implement dynamic programming as an efficient solution approach to an industrial control problem.

This course teaches you the key concepts of Reinforcement Learning, underlying classic and modern algorithms in RL. After completing this course, you will be able to start using RL for real problems, where you have or can specify the MDP.

## An Introduction to Sequential Decision-Making

For the first week of this course, you will learn how to understand the exploration-exploitation trade-off in sequential decision-making, implement incremental algorithms for estimating action-values, and compare the strengths and weaknesses to different algorithms for exploration. For this week’s graded assessment, you will implement and test an epsilon-greedy agent.

## Markov Decision Processes

The definition of MDPs, you will understand goal-directed behavior and how this can be obtained from maximizing scalar rewards, and you will also understand the difference between episodic and continuing tasks. For this week’s graded assessment, you will create three example tasks of your own that fit into the MDP framework.

## Value Functions & Bellman Equations

The definition of policies and value functions, as well as Bellman equations, which is the key technology that all of our algorithms will use.

## Dynamic Programming

How to compute value functions and optimal policies, assuming you have the MDP model. You will implement dynamic programming to compute value functions and optimal policies and understand the utility of dynamic programming for industrial applications and problems. Further, you will learn about Generalized Policy Iteration as a common template for constructing algorithms that maximize reward. For this week’s graded assessment, you will implement an efficient dynamic programming agent in a simulated industrial control problem.
